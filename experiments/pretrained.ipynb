{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f6141f1-af58-45a1-99cf-1dfeb2a50eb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9789c9f0-f551-4968-addc-16fc4b4ab993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models import VCN32, VCN16, VCN8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6eeb88-966b-4efd-b71f-71b4c11f8db1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Transfer Learning\n",
    "We will run experiments on VCN32, VCN16, and VCN8 to show the effect of transfer learning on training loss convergence. Transfer learning is taking a pretrained model and using it on a similar but different problem. In our case, the encoder backbone of the Fully Convolutional Networks (VCN) uses the convolutional layers of the VGG 11 architecture. \n",
    "\n",
    "Transfer learning helps speed up the training time of a model because it has already learned relevant features that may be applied to the current problem we're solving. If you know how to draw a cat, chances are you won't have difficulty learning how to draw a dog (hence the learnings from drawing a cat is \"transferable\"). To see the effects of transfer learning on training speed, we will train each FCN for 50 epochs on 3 configurations: \n",
    "\n",
    "1. <b>Encoder backbone without pretrained VGG 11</b>\n",
    "2. <b>Encoder backbone = pretrained VGG 11</b>\n",
    "3. <b>Encoder backbone = pretrained VGG 11 with weights frozen</b>\n",
    "\n",
    "We will produce training loss curves and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f18e65-2495-416c-b05b-62b0e53f2fb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## VCN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c628eb65-7ff4-494f-90f0-f3c00575ef62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vcn32s = [(\"not pretrained\", VCN32(pretrained=False)), \n",
    "          (\"pretrained\", VCN32(pretrained=True)), \n",
    "          (\"pretrained [frozen]\", VCN32(pretrained=True, freeze_pretrained=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47646b71-795e-47f1-897d-b0254bd241fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('not pretrained',\n",
       " VCN32(\n",
       "   (encoder): VGG11Encoder(\n",
       "     (down1): DownSample(\n",
       "       (conv_pool): Sequential(\n",
       "         (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (1): ReLU()\n",
       "         (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "       )\n",
       "     )\n",
       "     (down2): DownSample(\n",
       "       (conv_pool): Sequential(\n",
       "         (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (1): ReLU()\n",
       "         (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "       )\n",
       "     )\n",
       "     (down3): DownSample(\n",
       "       (conv_pool): Sequential(\n",
       "         (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (1): ReLU()\n",
       "         (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (3): ReLU()\n",
       "         (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "       )\n",
       "     )\n",
       "     (down4): DownSample(\n",
       "       (conv_pool): Sequential(\n",
       "         (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (1): ReLU()\n",
       "         (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (3): ReLU()\n",
       "         (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "       )\n",
       "     )\n",
       "     (down5): DownSample(\n",
       "       (conv_pool): Sequential(\n",
       "         (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (1): ReLU()\n",
       "         (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "         (3): ReLU()\n",
       "         (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "       )\n",
       "     )\n",
       "     (prediction): Sequential(\n",
       "       (0): Conv2d(512, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (1): ReLU()\n",
       "       (2): Dropout2d(p=0.5, inplace=False)\n",
       "       (3): Conv2d(4096, 4096, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "       (4): ReLU()\n",
       "       (5): Dropout2d(p=0.5, inplace=False)\n",
       "       (6): Conv2d(4096, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "     )\n",
       "   )\n",
       "   (up32): UpSample(\n",
       "     (conv_tranpose): ConvTranspose2d(2, 2, kernel_size=(64, 64), stride=(32, 32), padding=(16, 16))\n",
       "   )\n",
       " ))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcn32s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b5d6b10e-55f0-4f43-ac89-6f10281a8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "shapes = [(1, 3, 112, 112), (1, 3, 945, 673),\n",
    "          (1, 3, 448, 448), (1, 3, 567, 345), (1, 3, 452, 224)]\n",
    "\n",
    "model = VCN32()\n",
    "for shape in shapes:\n",
    "    inp = torch.rand(shape)\n",
    "    out = model(inp)\n",
    "\n",
    "    assert out.shape[2:] == inp.shape[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc8f1d-f150-4079-88a8-8d314ba582c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
